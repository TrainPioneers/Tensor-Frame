<!DOCTYPE HTML>
<html lang="en" class="navy sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CUDA Backend - Tensor Frame Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="A high-performance, PyTorch-like tensor library for Rust with multiple computational backends">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "navy";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('navy')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Tensor Frame Documentation</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/TrainPioneers/Tensor-Frame" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="cuda-backend"><a class="header" href="#cuda-backend">CUDA Backend</a></h1>
<p>The CUDA backend provides high-performance tensor operations on NVIDIA GPUs using the CUDA toolkit. It offers the highest performance for supported operations and integrates well with the broader CUDA ecosystem.</p>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<ul>
<li><strong>Peak Performance</strong>: Optimized kernels for maximum NVIDIA GPU utilization</li>
<li><strong>Optimized Kernels</strong>: Hardware-accelerated tensor operations</li>
<li><strong>Memory Optimization</strong>: Efficient GPU memory management</li>
<li><strong>Mature Ecosystem</strong>: Integration with existing CUDA libraries</li>
<li><strong>Production Ready</strong>: Battle-tested in production environments</li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<h3 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h3>
<p><strong>CUDA Toolkit</strong>: Install NVIDIA CUDA Toolkit 11.0 or later</p>
<ul>
<li>Download from <a href="https://developer.nvidia.com/cuda-toolkit">NVIDIA Developer</a></li>
<li>Ensure <code>nvcc</code> is in your PATH</li>
<li>Verify installation: <code>nvcc --version</code></li>
</ul>
<p><strong>Compatible GPU</strong>: NVIDIA GPU with compute capability 3.5+</p>
<ul>
<li>Check compatibility: <code>nvidia-smi</code></li>
<li>Verify compute capability: <code>deviceQuery</code> (CUDA samples)</li>
</ul>
<h3 id="cargo-configuration"><a class="header" href="#cargo-configuration">Cargo Configuration</a></h3>
<p>Enable the CUDA backend:</p>
<pre><code class="language-toml">[dependencies]
tensor_frame = { version = "0.0.3-alpha", features = ["cuda"] }
</code></pre>
<p><strong>Build Requirements</strong>:</p>
<ul>
<li>CUDA Toolkit installed</li>
<li>NVIDIA GPU drivers</li>
<li>C++ compiler (MSVC on Windows, GCC/Clang on Linux)</li>
</ul>
<h2 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h2>
<h3 id="hardware"><a class="header" href="#hardware">Hardware</a></h3>
<ul>
<li><strong>GPU</strong>: NVIDIA GPU with compute capability 3.5+</li>
<li><strong>Memory</strong>: Sufficient GPU memory for tensor operations</li>
<li><strong>PCIe</strong>: PCIe 3.0 x16 recommended for optimal memory bandwidth</li>
</ul>
<h3 id="software"><a class="header" href="#software">Software</a></h3>
<ul>
<li><strong>CUDA Toolkit</strong>: Version 11.0+ (12.0+ recommended)</li>
<li><strong>Driver</strong>: NVIDIA driver supporting your CUDA version</li>
<li><strong>OS</strong>: Linux (preferred), Windows 10+, WSL2</li>
</ul>
<h3 id="verified-configurations"><a class="header" href="#verified-configurations">Verified Configurations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>GPU Generation</th><th>Compute Capability</th><th>CUDA Version</th><th>Status</th></tr></thead><tbody>
<tr><td>Maxwell (GTX 900)</td><td>5.0, 5.2</td><td>11.0+</td><td>✅ Supported</td></tr>
<tr><td>Pascal (GTX 10x0)</td><td>6.0, 6.1</td><td>11.0+</td><td>✅ Fully supported</td></tr>
<tr><td>Volta (V100)</td><td>7.0</td><td>11.0+</td><td>✅ Optimized</td></tr>
<tr><td>Turing (RTX 20x0)</td><td>7.5</td><td>11.0+</td><td>✅ Optimized</td></tr>
<tr><td>Ampere (RTX 30x0)</td><td>8.0, 8.6</td><td>11.2+</td><td>✅ Optimal</td></tr>
<tr><td>Ada (RTX 40x0)</td><td>8.9</td><td>12.0+</td><td>✅ Latest features</td></tr>
</tbody></table>
</div>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<h3 id="storage"><a class="header" href="#storage">Storage</a></h3>
<p>CUDA tensors use device memory pointers:</p>
<pre><code class="language-rust">pub struct CudaStorage {
    pub ptr: *mut f32,    // Raw CUDA device pointer
    pub len: usize,       // Buffer length in elements
}</code></pre>
<p><strong>Memory Properties</strong>:</p>
<ul>
<li><strong>Location</strong>: GPU global memory (VRAM)</li>
<li><strong>Layout</strong>: Contiguous, row-major layout</li>
<li><strong>Alignment</strong>: 256-byte aligned for optimal coalescing</li>
<li><strong>Synchronization</strong>: Explicit via CUDA streams</li>
</ul>
<h3 id="kernel-implementation"><a class="header" href="#kernel-implementation">Kernel Implementation</a></h3>
<p>Operations use optimized CUDA kernels:</p>
<pre><code class="language-cuda">// Element-wise addition kernel
__global__ void add_kernel(const float* a, const float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx &lt; n) {
        c[idx] = a[idx] + b[idx];
    }
}
</code></pre>
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<h3 id="strengths"><a class="header" href="#strengths">Strengths</a></h3>
<ul>
<li><strong>Compute Throughput</strong>: Maximum FP32/FP16 throughput on NVIDIA hardware</li>
<li><strong>Memory Bandwidth</strong>: Optimal utilization of GPU memory bandwidth</li>
<li><strong>Kernel Optimization</strong>: Hand-tuned kernels for each operation</li>
<li><strong>Library Integration</strong>: Designed for future integration with cuDNN, etc.</li>
</ul>
<h3 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h3>
<p>Example performance on RTX 4090:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Tensor Size</th><th>CPU (32 cores)</th><th>CUDA</th><th>Speedup</th></tr></thead><tbody>
<tr><td>Element-wise Add</td><td>1M elements</td><td>2.1 ms</td><td>0.18 ms</td><td>12x</td></tr>
<tr><td>Matrix Multiply</td><td>2048x2048</td><td>450 ms</td><td>8.2 ms</td><td>55x</td></tr>
<tr><td>Reduction Sum</td><td>16M elements</td><td>15 ms</td><td>0.52 ms</td><td>29x</td></tr>
</tbody></table>
</div>
<h3 id="optimization-guidelines"><a class="header" href="#optimization-guidelines">Optimization Guidelines</a></h3>
<h4 id="optimal-use-cases"><a class="header" href="#optimal-use-cases">Optimal Use Cases</a></h4>
<pre><code class="language-rust">// Large tensor operations
let a = Tensor::zeros(vec![4096, 4096])?;
let b = Tensor::zeros(vec![4096, 4096])?;
let c = (a * b) + 1.0;  // Excellent GPU performance

// Batch operations
for batch in large_dataset {
    let result = model.forward(batch)?;  // Amortizes GPU overhead
}

// Memory-bound operations
let result = ((a * b) + c) / d;  // GPU memory bandwidth utilized</code></pre>
<h4 id="suboptimal-use-cases"><a class="header" href="#suboptimal-use-cases">Suboptimal Use Cases</a></h4>
<pre><code class="language-rust">// Very small tensors
let tiny = Tensor::ones(vec![8, 8])?;  // Kernel launch overhead dominates

// Frequent host-device transfers
let gpu_result = cpu_tensor.to_backend(BackendType::Cuda)?;
let back_to_cpu = gpu_result.to_vec()?;  // PCIe bandwidth bottleneck

// Scalar reductions with immediate use
let sum = tensor.sum(None)?.to_vec()?;  // Forces synchronization</code></pre>
<h2 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h2>
<h3 id="device-memory-allocation"><a class="header" href="#device-memory-allocation">Device Memory Allocation</a></h3>
<p>CUDA tensors allocate GPU memory directly:</p>
<pre><code class="language-rust">// Allocates 64MB of GPU memory
let large_tensor = Tensor::zeros(vec![4096, 4096])?
    .to_backend(BackendType::Cuda)?;</code></pre>
<h3 id="memory-pool-management"><a class="header" href="#memory-pool-management">Memory Pool Management</a></h3>
<p>The backend uses a memory pool for efficient allocation:</p>
<pre><code class="language-rust">// Pool reduces allocation overhead
let tensors: Vec&lt;Tensor&gt; = (0..100)
    .map(|_| Tensor::zeros(vec![1024, 1024]))
    .collect::&lt;Result&lt;Vec&lt;_&gt;&gt;&gt;()?;</code></pre>
<h3 id="memory-transfer-optimization"><a class="header" href="#memory-transfer-optimization">Memory Transfer Optimization</a></h3>
<pre><code class="language-rust">// Efficient: Batch transfers
let gpu_tensors = cpu_tensors
    .into_iter()
    .map(|t| t.to_backend(BackendType::Cuda))
    .collect::&lt;Result&lt;Vec&lt;_&gt;&gt;&gt;()?;

// Inefficient: Individual transfers  
for cpu_tensor in cpu_tensors {
    let gpu_tensor = cpu_tensor.to_backend(BackendType::Cuda)?;
    process(gpu_tensor)?;
}</code></pre>
<h3 id="memory-debugging"><a class="header" href="#memory-debugging">Memory Debugging</a></h3>
<p>Monitor GPU memory usage:</p>
<pre><code class="language-bash"># Check GPU memory
nvidia-smi

# Continuous monitoring
watch -n 1 nvidia-smi
</code></pre>
<pre><code class="language-rust">// Check available memory
let (free, total) = cuda::memory_info()?;
println!("GPU memory: {}/{} MB", free / 1024 / 1024, total / 1024 / 1024);

// Handle out-of-memory
match Tensor::zeros(vec![16384, 16384]).and_then(|t| t.to_backend(BackendType::Cuda)) {
    Ok(tensor) =&gt; println!("Allocated 1GB GPU tensor"),
    Err(TensorError::BackendError(msg)) if msg.contains("memory") =&gt; {
        eprintln!("GPU OOM, trying smaller allocation");
    }
    Err(e) =&gt; eprintln!("CUDA error: {}", e),
}</code></pre>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>CUDA operations can fail for various hardware and software reasons:</p>
<h3 id="runtime-errors"><a class="header" href="#runtime-errors">Runtime Errors</a></h3>
<pre><code class="language-rust">use tensor_frame::{Tensor, TensorError};

match tensor_operation() {
    Ok(result) =&gt; process(result),
    Err(TensorError::BackendError(msg)) =&gt; {
        if msg.contains("out of memory") {
            // GPU memory exhausted
            fallback_to_cpu()?;
        } else if msg.contains("invalid device") {
            // GPU not available or driver issue
            retry_with_cpu_backend()?;
        } else {
            // Other CUDA error
            eprintln!("CUDA error: {}", msg);
        }
    }
}</code></pre>
<h3 id="common-error-scenarios"><a class="header" href="#common-error-scenarios">Common Error Scenarios</a></h3>
<ul>
<li><strong>GPU Out of Memory</strong>: Tensor too large for available GPU memory</li>
<li><strong>Invalid Device</strong>: GPU not found or not compatible</li>
<li><strong>Driver Mismatch</strong>: CUDA driver version incompatible</li>
<li><strong>Kernel Launch Failed</strong>: Invalid kernel parameters or GPU fault</li>
<li><strong>Memory Access Violation</strong>: Invalid GPU memory access</li>
</ul>
<h3 id="error-recovery"><a class="header" href="#error-recovery">Error Recovery</a></h3>
<pre><code class="language-rust">// Graceful fallback strategy
fn robust_tensor_operation(tensor: Tensor) -&gt; Result&lt;Tensor&gt; {
    // Try CUDA first
    if let Ok(cuda_tensor) = tensor.to_backend(BackendType::Cuda) {
        match cuda_operation(cuda_tensor) {
            Ok(result) =&gt; return Ok(result),
            Err(TensorError::BackendError(_)) =&gt; {
                // CUDA failed, fall back to CPU
                eprintln!("CUDA operation failed, falling back to CPU");
            }
        }
    }
    
    // CPU fallback
    cpu_operation(tensor.to_backend(BackendType::Cpu)?)
}</code></pre>
<h2 id="debugging-and-profiling"><a class="header" href="#debugging-and-profiling">Debugging and Profiling</a></h2>
<h3 id="cuda-debugging-tools"><a class="header" href="#cuda-debugging-tools">CUDA Debugging Tools</a></h3>
<p><strong>NVIDIA Nsight Systems</strong>: System-wide performance analysis</p>
<pre><code class="language-bash">nsys profile --stats=true ./your_app
</code></pre>
<p><strong>NVIDIA Nsight Compute</strong>: Kernel-level profiling</p>
<pre><code class="language-bash">ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed ./your_app
</code></pre>
<p><strong>cuda-memcheck</strong>: Memory error detection</p>
<pre><code class="language-bash">cuda-memcheck ./your_app
</code></pre>
<h3 id="performance-analysis"><a class="header" href="#performance-analysis">Performance Analysis</a></h3>
<pre><code class="language-rust">// GPU timing with CUDA events
use std::time::Instant;

let start = Instant::now();
let result = gpu_tensor_a.matmul(&amp;gpu_tensor_b)?;
// Note: matmul is asynchronous!
let _sync = result.to_vec()?;  // Force synchronization
let elapsed = start.elapsed();
println!("Matrix multiplication took: {:?}", elapsed);</code></pre>
<h3 id="memory-leak-detection"><a class="header" href="#memory-leak-detection">Memory Leak Detection</a></h3>
<pre><code class="language-rust">// Monitor for memory leaks in long-running applications
fn check_memory_usage() -&gt; Result&lt;()&gt; {
    let (free_before, total) = cuda::memory_info()?;
    
    // Perform operations
    {
        let tensor = Tensor::zeros(vec![1000, 1000])?.to_backend(BackendType::Cuda)?;
        let result = expensive_operation(tensor)?;
    } // tensor should be freed here
    
    let (free_after, _) = cuda::memory_info()?;
    
    if free_after &lt; free_before {
        eprintln!("Potential memory leak detected!");
        eprintln!("Memory delta: {} MB", (free_before - free_after) / 1024 / 1024);
    }
    
    Ok(())
}</code></pre>
<h2 id="production-deployment"><a class="header" href="#production-deployment">Production Deployment</a></h2>
<h3 id="docker-configuration"><a class="header" href="#docker-configuration">Docker Configuration</a></h3>
<pre><code class="language-dockerfile"># Use NVIDIA CUDA base image
FROM nvidia/cuda:12.0-devel-ubuntu20.04

# Install Rust
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"

# Copy and build your application
COPY . /app
WORKDIR /app
RUN cargo build --release --features cuda

# Runtime with CUDA
FROM nvidia/cuda:12.0-runtime-ubuntu20.04
COPY --from=0 /app/target/release/your_app /usr/local/bin/
CMD ["your_app"]
</code></pre>
<h3 id="kubernetes-deployment"><a class="header" href="#kubernetes-deployment">Kubernetes Deployment</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
spec:
  containers:
  - name: tensor-app
    image: your-app:latest
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: CUDA_VISIBLE_DEVICES
      value: "0"
</code></pre>
<h3 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h3>
<pre><code class="language-bash"># Limit GPU memory growth
export CUDA_MEMORY_POOL_TYPE=pool

# Enable GPU timing
export CUDA_LAUNCH_BLOCKING=1

# Select specific GPU
export CUDA_VISIBLE_DEVICES=0
</code></pre>
<h2 id="optimization-best-practices"><a class="header" href="#optimization-best-practices">Optimization Best Practices</a></h2>
<h3 id="memory-access-patterns"><a class="header" href="#memory-access-patterns">Memory Access Patterns</a></h3>
<pre><code class="language-rust">// Coalesced memory access (efficient)
let result = tensor_a + tensor_b;  // Sequential element access

// Strided access (less efficient)
let transposed = tensor.transpose()?;  // May require memory reshape</code></pre>
<h3 id="kernel-fusion"><a class="header" href="#kernel-fusion">Kernel Fusion</a></h3>
<pre><code class="language-rust">// Fused operations (single kernel launch)
let result = ((a * b) + c).relu();  // Ideally fused into one kernel

// Separate operations (multiple kernel launches)
let temp1 = a * b;
let temp2 = temp1 + c;
let result = temp2.relu();  // Three separate kernels</code></pre>
<h3 id="stream-management"><a class="header" href="#stream-management">Stream Management</a></h3>
<pre><code class="language-rust">// Future: Async operations with CUDA streams
// Currently synchronous, but optimizations planned
let stream_a = cuda::create_stream()?;
let stream_b = cuda::create_stream()?;

// Parallel execution on different streams
let result_a = tensor_a.sum(None).execute_on(stream_a)?;
let result_b = tensor_b.mean(None).execute_on(stream_b)?;</code></pre>
<h2 id="integration-with-cuda-ecosystem"><a class="header" href="#integration-with-cuda-ecosystem">Integration with CUDA Ecosystem</a></h2>
<h3 id="cudnn-future"><a class="header" href="#cudnn-future">cuDNN (Future)</a></h3>
<p>Planned integration for neural network operations:</p>
<pre><code class="language-rust">// Future: Convolution operations
let output = input.conv2d(&amp;kernel, stride, padding)?;</code></pre>
<h3 id="nccl-future"><a class="header" href="#nccl-future">NCCL (Future)</a></h3>
<p>Multi-GPU communication for distributed computing:</p>
<pre><code class="language-rust">// Future: Multi-GPU operations
let distributed_result = tensor.all_reduce_sum()?;</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../backends/wgpu.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../examples/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../backends/wgpu.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../examples/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
